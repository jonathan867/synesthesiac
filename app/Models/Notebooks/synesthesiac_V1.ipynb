{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ep2ZaJRNxw4_"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df = pd.read_csv(\"train_dataset.csv\", usecols = ['title', 'artist', 'genre', 'general_genre', 'emotion_4Q', 'emotion_2Q', 'lyrics'])\n",
        "train_df = pd.read_csv(\"train_dataset.csv\", usecols = ['emotion_4Q', 'emotion_2Q', 'lyrics'])\n",
        "val_df = pd.read_csv(\"val_dataset.csv\", usecols = ['emotion_4Q', 'emotion_2Q', 'lyrics'])\n",
        "test_df = pd.read_csv(\"test_dataset.csv\", usecols = ['emotion_4Q', 'emotion_2Q', 'lyrics'])"
      ],
      "metadata": {
        "id": "O0BSHOMwyvNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train_df.head()"
      ],
      "metadata": {
        "id": "J9VDBaXu3cOh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing\n",
        "# Convert emotion labels to one-hot encoding\n",
        "train_labels = pd.get_dummies(train_df['emotion_4Q'])\n",
        "val_labels = pd.get_dummies(val_df['emotion_4Q'])\n",
        "test_labels = pd.get_dummies(test_df['emotion_4Q'])\n",
        "\n",
        "# Convert lyrics to numpy arrays\n",
        "train_lyrics = np.array(train_df['lyrics'])\n",
        "val_lyrics = np.array(val_df['lyrics'])\n",
        "test_lyrics = np.array(test_df['lyrics'])"
      ],
      "metadata": {
        "id": "dOrhoAtE5jXB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating TensorFlow Dataset Objects\n",
        "def df_to_dataset(dataframe, labels, shuffle=True, batch_size=1024):\n",
        "    ds = tf.data.Dataset.from_tensor_slices((dataframe, labels))\n",
        "    if shuffle:\n",
        "        ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "    ds = ds.batch(batch_size)\n",
        "    ds = ds.prefetch(tf.data.AUTOTUNE)\n",
        "    return ds\n",
        "\n",
        "train_data = df_to_dataset(train_lyrics, train_labels)\n",
        "val_data = df_to_dataset(val_lyrics, val_labels)\n",
        "test_data = df_to_dataset(test_lyrics, test_labels)"
      ],
      "metadata": {
        "id": "6J2efF9qGVAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Download NLTK stopwords\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Get English stopwords\n",
        "stop_words = set(stopwords.words('english'))\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove text within square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters and extra whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    # Remove stopwords\n",
        "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
        "    return text.strip()\n",
        "\n",
        "# Apply the cleaning function to each lyric in the dataset\n",
        "train_df['clean_lyrics'] = train_df['lyrics'].apply(clean_text)\n",
        "val_df['clean_lyrics'] = val_df['lyrics'].apply(clean_text)\n",
        "test_df['clean_lyrics'] = test_df['lyrics'].apply(clean_text)\n",
        "\n",
        "# Re-create TensorFlow Dataset objects with cleaned lyrics\n",
        "train_data = df_to_dataset(train_df['clean_lyrics'], train_labels)\n",
        "val_data = df_to_dataset(val_df['clean_lyrics'], val_labels)\n",
        "test_data = df_to_dataset(test_df['clean_lyrics'], test_labels)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8RuIQ3L57Vl",
        "outputId": "b9660c5f-2fc0-4c56-c043-72a4d39cf9f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Print a few lines of cleaned lyrics data\n",
        "print(\"Sample cleaned lyrics from training dataset:\")\n",
        "for i in range(5):\n",
        "    print(train_df['clean_lyrics'].iloc[i])\n",
        "    print('-' * 50)"
      ],
      "metadata": {
        "id": "n9sEyCgH6VOn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text Embedding with TensorFlow Hub\n",
        "embedding = \"https://tfhub.dev/google/universal-sentence-encoder/4\"\n",
        "hub_layer = hub.KerasLayer(embedding, dtype=tf.string, trainable=True)"
      ],
      "metadata": {
        "id": "F4GU_yd9G3Sz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Definition and Compilation\n",
        "model = tf.keras.Sequential()\n",
        "model.add(hub_layer)\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(16, activation='relu'))\n",
        "model.add(tf.keras.layers.Dropout(0.4))\n",
        "model.add(tf.keras.layers.Dense(train_labels.shape[1], activation='softmax'))  # Output layer for multi-class classification\n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "t6iG9Vo0H1zn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training  (model version 1)\n",
        "history = model.fit(train_data, epochs=8, validation_data=val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lemz5qVFIJpN",
        "outputId": "f12f4eec-35e1-4a28-eee7-97b760a5b424"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "2/2 [==============================] - 40s 9s/step - loss: 1.3872 - accuracy: 0.2491 - val_loss: 1.3759 - val_accuracy: 0.3481\n",
            "Epoch 2/8\n",
            "2/2 [==============================] - 19s 9s/step - loss: 1.3715 - accuracy: 0.3331 - val_loss: 1.3586 - val_accuracy: 0.3993\n",
            "Epoch 3/8\n",
            "2/2 [==============================] - 19s 9s/step - loss: 1.3617 - accuracy: 0.3272 - val_loss: 1.3434 - val_accuracy: 0.4300\n",
            "Epoch 4/8\n",
            "2/2 [==============================] - 18s 10s/step - loss: 1.3501 - accuracy: 0.3419 - val_loss: 1.3319 - val_accuracy: 0.4437\n",
            "Epoch 5/8\n",
            "2/2 [==============================] - 19s 9s/step - loss: 1.3342 - accuracy: 0.3601 - val_loss: 1.3193 - val_accuracy: 0.4608\n",
            "Epoch 6/8\n",
            "2/2 [==============================] - 18s 9s/step - loss: 1.3109 - accuracy: 0.4164 - val_loss: 1.3043 - val_accuracy: 0.4710\n",
            "Epoch 7/8\n",
            "2/2 [==============================] - 20s 10s/step - loss: 1.2905 - accuracy: 0.4346 - val_loss: 1.2897 - val_accuracy: 0.4744\n",
            "Epoch 8/8\n",
            "2/2 [==============================] - 19s 9s/step - loss: 1.2871 - accuracy: 0.4302 - val_loss: 1.2766 - val_accuracy: 0.4949\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7oKfeuViIQiw",
        "outputId": "c18b261a-ac64-4078-93ad-0a2dd2467bfa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 408ms/step - loss: 1.2744 - accuracy: 0.5238\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2744420766830444, 0.523809552192688]"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "clQPnNdqMPfP",
        "outputId": "8028e371-7da1-4dc6-94c7-3d867100afba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 341ms/step - loss: 1.2766 - accuracy: 0.4949\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2766435146331787, 0.4948805570602417]"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71OP51k8MbtP",
        "outputId": "65a17771-ca18-4485-b954-0979745c5ddd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 2s 387ms/step - loss: 1.2381 - accuracy: 0.6245\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.2381280660629272, 0.6245434880256653]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM"
      ],
      "metadata": {
        "id": "j1CisAwkLjCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "\n",
        "def clean_text(text):\n",
        "    # Remove text within square brackets\n",
        "    text = re.sub(r'\\[.*?\\]', '', text)\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove non-alphanumeric characters and extra whitespaces\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    return text.strip()\n",
        "\n",
        "# Apply the cleaning function to each lyric in the dataset\n",
        "train_df['clean_lyrics'] = train_df['lyrics'].apply(clean_text)\n",
        "val_df['clean_lyrics'] = val_df['lyrics'].apply(clean_text)\n",
        "test_df['clean_lyrics'] = test_df['lyrics'].apply(clean_text)\n",
        "\n",
        "# Re-create TensorFlow Dataset objects with cleaned lyrics\n",
        "train_data = df_to_dataset(train_df['clean_lyrics'], train_labels)\n",
        "val_data = df_to_dataset(val_df['clean_lyrics'], val_labels)\n",
        "test_data = df_to_dataset(test_df['clean_lyrics'], test_labels)"
      ],
      "metadata": {
        "id": "zFetqRgfQ4bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LSTM Model\n",
        "encoder = tf.keras.layers.TextVectorization(max_tokens=2000)\n",
        "encoder.adapt(train_data.map(lambda text, label: text))"
      ],
      "metadata": {
        "id": "joUq1iinKuAq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab_size = len(encoder.get_vocabulary())\n",
        "vocab = np.array(encoder.get_vocabulary())\n",
        "vocab[:50]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sEHpUR0QORP8",
        "outputId": "c298cac7-d08e-4a53-eb67-df44bca82f71"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['', '[UNK]', 'the', 'you', 'i', 'and', 'to', 'a', 'me', 'it', 'my',\n",
              "       'in', 'of', 'your', 'on', 'im', 'that', 'is', 'all', 'we', 'for',\n",
              "       'be', 'so', 'dont', 'its', 'no', 'like', 'with', 'just', 'up',\n",
              "       'but', 'what', 'love', 'oh', 'this', 'know', 'now', 'got', 'can',\n",
              "       'if', 'when', 'out', 'do', 'go', 'youre', 'down', 'yeah', 'get',\n",
              "       'are', 'come'], dtype='<U13')"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = tf.keras.Sequential([\n",
        "    encoder,\n",
        "    tf.keras.layers.Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=32,\n",
        "        mask_zero=True\n",
        "    ),\n",
        "    tf.keras.layers.LSTM(32),\n",
        "    tf.keras.layers.Dense(32, activation='relu'),\n",
        "    tf.keras.layers.Dropout(0.4),\n",
        "    tf.keras.layers.Dense(train_labels.shape[1], activation='softmax')  # Output layer for multi-class classification\n",
        "])\n"
      ],
      "metadata": {
        "id": "bPWfBzrOOhPN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
        "              loss=tf.keras.losses.CategoricalCrossentropy(),\n",
        "              metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "qaZDrN4YOuGU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fb9lZucBOwpO",
        "outputId": "77b49294-576e-4649-d983-71b95967360e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " text_vectorization (TextVe  (None, None)              0         \n",
            " ctorization)                                                    \n",
            "                                                                 \n",
            " embedding (Embedding)       (None, None, 32)          64000     \n",
            "                                                                 \n",
            " lstm (LSTM)                 (None, 32)                8320      \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 32)                1056      \n",
            "                                                                 \n",
            " dropout_2 (Dropout)         (None, 32)                0         \n",
            "                                                                 \n",
            " dense_4 (Dense)             (None, 4)                 132       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 73508 (287.14 KB)\n",
            "Trainable params: 73508 (287.14 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(train_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDoupey6O-oo",
        "outputId": "3d1738f3-3b94-49ea-f443-bb5ab465bd79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2/2 [==============================] - 7s 679ms/step - loss: 1.3861 - accuracy: 0.2367\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3861325979232788, 0.23666910827159882]"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9JCI6t-O_Rf",
        "outputId": "1921d522-dbb5-4836-9372-bc279d8a7a3e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 414ms/step - loss: 1.3870 - accuracy: 0.2014\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3869799375534058, 0.20136518776416779]"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Training\n",
        "history = model.fit(train_data, epochs=8, validation_data=val_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTh98NVMOzz9",
        "outputId": "1d907d68-3a4a-4fd9-9c31-594c10942580"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/8\n",
            "2/2 [==============================] - 7s 2s/step - loss: 1.3852 - accuracy: 0.2812 - val_loss: 1.3845 - val_accuracy: 0.2935\n",
            "Epoch 2/8\n",
            "2/2 [==============================] - 7s 2s/step - loss: 1.3838 - accuracy: 0.3075 - val_loss: 1.3838 - val_accuracy: 0.2833\n",
            "Epoch 3/8\n",
            "2/2 [==============================] - 7s 3s/step - loss: 1.3825 - accuracy: 0.3002 - val_loss: 1.3831 - val_accuracy: 0.2867\n",
            "Epoch 4/8\n",
            "2/2 [==============================] - 7s 2s/step - loss: 1.3809 - accuracy: 0.2958 - val_loss: 1.3823 - val_accuracy: 0.2867\n",
            "Epoch 5/8\n",
            "2/2 [==============================] - 8s 2s/step - loss: 1.3793 - accuracy: 0.3002 - val_loss: 1.3815 - val_accuracy: 0.2867\n",
            "Epoch 6/8\n",
            "2/2 [==============================] - 8s 2s/step - loss: 1.3777 - accuracy: 0.3039 - val_loss: 1.3806 - val_accuracy: 0.2867\n",
            "Epoch 7/8\n",
            "2/2 [==============================] - 6s 2s/step - loss: 1.3756 - accuracy: 0.3031 - val_loss: 1.3796 - val_accuracy: 0.2867\n",
            "Epoch 8/8\n",
            "2/2 [==============================] - 8s 2s/step - loss: 1.3724 - accuracy: 0.3039 - val_loss: 1.3784 - val_accuracy: 0.2867\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model Evaluation\n",
        "model.evaluate(test_data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G6ZuJyz0O1um",
        "outputId": "f934ef61-665f-4c28-a500-2e919295fc68"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 574ms/step - loss: 1.3779 - accuracy: 0.2891\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.3779428005218506, 0.28911563754081726]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ]
}